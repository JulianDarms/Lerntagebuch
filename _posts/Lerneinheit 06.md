---
title: "Lerneinheit 8 und 9: Metadaten modellieren und Schnittstellen nutzen A (OpenRefine) (Teil 1/3 + 2/3) "
date: 2024-04-30
---

In den vergangenen Sitzungen am 23.04. und 30.04. haben wir uns intensiv mit dem Thema Metadatenmodellierung und der Nutzung von Schnittstellen beschäftigt. Im Fokus stand dabei das Open-Source-Tool OpenRefine, das sich als äusserst nützlich für Buchhändler erweisen kann, insbesondere für diejenigen, die mit umfangreichen und teils chaotischen Datensätzen arbeiten.
Einführung in OpenRefine
OpenRefine ist ein leistungsstarkes, kostenloses Werkzeug, das speziell zur Arbeit mit unstrukturierten und „chaotischen“ Daten entwickelt wurde. Es ähnelt in seiner grafischen Oberfläche klassischen Tabellenverarbeitungsprogrammen wie Excel, bietet jedoch wesentlich erweiterte Funktionen zur Datenanalyse, -bereinigung, -konvertierung und -anreicherung. Das Tool wird lokal installiert und über den Browser bedient, was bedeutet, dass es auch sicher für die Verarbeitung vertraulicher Daten verwendet werden kann.
Basisfunktionen: Facetten, Undo/Redo, Clustering
Eine der ersten Aufgaben bestand darin, ein Projekt zu erstellen und Beispieldaten aus dem Internet zu laden. Wir haben eine CSV-Datei mit Artikeldaten importiert, was durch die einfache Handhabung von OpenRefine gut funktionierte. Die geladenen Daten wurden in einer übersichtlichen Tabelle dargestellt, die uns einen klaren Überblick verschaffte.
Facetten: Mit Facetten haben wir uns verschiedene Kategorien innerhalb der Daten anzeigen lassen, z.B. die unterschiedlichen Sprachen in den Artikeldaten. Dies erleichtert das Auffinden und Bearbeiten von Datenkategorien erheblich. So konnten wir schnell erkennen, dass es unterschiedliche Abkürzungen für dieselbe Sprache gab (z.B. „EN“ und „English“), die wir dann vereinheitlicht haben.
Undo/Redo: Jede Änderung in OpenRefine wird automatisch in einer Historie protokolliert. Diese Funktion ermöglichte es uns, beliebig viele Schritte rückgängig zu machen oder wiederherzustellen, was die Arbeit sehr flexibel und sicher gestaltete.
Clustering: Mit der Clusterfunktion konnten wir ähnliche, aber unterschiedlich geschriebene Einträge gruppieren und vereinheitlichen. Beispielsweise wurden Autoren, die in verschiedenen Schreibweisen auftauchten, mittels Clustering zusammengeführt. Dies spart enorm viel Zeit und sorgt für eine konsistente Datendarstellung.
Vorführung der Reconciliation-Funktion
Ein Highlight war die Demonstration der Reconciliation-Funktion. Diese ermöglicht es, Daten mit externen Quellen abzugleichen und anzureichern. Wir haben dies anhand von Zeitschriftenartikeln demonstriert, bei denen wir die ISSNs verwendet haben, um zusätzliche Informationen aus Wikidata abzurufen. Dies erfolgt über eine einfache Spaltenfunktion und ist besonders nützlich, um Daten schnell und zuverlässig zu validieren und anzureichern.
Für Buchhändler, die häufig mit grossen Mengen an bibliografischen Daten arbeiten, bietet diese Funktion eine erhebliche Erleichterung. Beispielsweise können abweichende Schreibweisen von Buchtiteln oder Autorennamen automatisch korrigiert und ergänzt werden, was die Qualität und Konsistenz der Daten in Warenwirtschaftssystemen deutlich verbessern würde.
Praktische Übungen
Im Rahmen der Übungen haben wir verschiedene Basisfunktionen von OpenRefine durchlaufen. Eine Übung bestand darin, eine Spalte mit mehrwertigen Zellen zu splitten und wieder zu vereinen. Dies ist besonders nützlich, wenn Daten aus verschiedenen Quellen zusammengeführt werden müssen. Eine weitere Übung zeigte uns, wie man durch den Abgleich mit Normdatenbanken wie Wikidata oder der GND (Gemeinsame Normdatei) Daten anreichern kann.
Reflexion 
Insgesamt hat uns der Kurs gezeigt, dass OpenRefine weit mehr als nur ein einfaches Tabellenkalkulationsprogramm ist. Die speziellen Funktionen zur Datenbereinigung und -anreicherung waren für mich besonders Interessant, da sie die Qualität der bibliografischen Daten erheblich verbessern können. Während Excel bei einfachen Tabellen und kleinen Datenmengen hilfreich ist, zeigt OpenRefine seine Stärken bei der Verarbeitung grosser und unstrukturierter Datensätze.
